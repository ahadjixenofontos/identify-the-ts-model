{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering time series to identify appropriate model\n",
    "\n",
    "Cluster the simulated series in order to identify similar subgroups. Do these subgroups correspond to the processes that generated each class of series? \n",
    "\n",
    "Since the actual class is known because the data is simulated, we are able to evaluate whether clustering the series can bin the series into the correct bins. \n",
    "\n",
    "Since the learned group labels are arbitrary, it is not as straightforward to calculate performance metrics as it would be for any given classifier. Evaluating the performance of the method is therefore done using cluster spillover. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tslearn\n",
    "import hdbscan \n",
    "\n",
    "# has warping path visualization functionality\n",
    "import dtaidistance # only calculates euclidean distance to speed up C-code underneath by avoiding extra function calls\n",
    "from dtaidistance import dtw_visualisation as dtwvis\n",
    "\n",
    "import fastdtw\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from time_series_simulations import time_series_simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_series = time_series_simulations.simulate_ts_population()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate pairwise distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.96744362 -0.60208952 -0.15950415 -0.92892953 -0.52892116 -0.85030903\n",
      " -0.54530486 -0.12788218  0.24844516  0.18930142  0.12199985  0.42065857\n",
      "  0.08978324  0.41917576 -0.29557224 -0.06200852  0.64187934  0.67407302\n",
      "  0.79140348  0.78430108  1.51479116  1.70722751  2.17344781  1.70129721\n",
      "  1.1486897   1.44275     0.93640735  1.18922432  1.19258737  1.40340918\n",
      "  0.77141647  0.59432187  0.27447929 -0.47317681 -0.38008576 -0.07814134\n",
      " -0.75180705 -1.18694318 -1.27460195 -1.93145574 -1.81212629 -1.58608328\n",
      " -1.18507232 -1.88669454 -1.51404165 -1.33595005 -0.21476333  0.36193506\n",
      "  0.06690189  0.23221634 -0.09639956 -0.31681578]\n"
     ]
    }
   ],
   "source": [
    "# dtaidistance has a built in function that produces a pairwise distance matrix \n",
    "# first put all the series in a list of lists \n",
    "series_list = standardized_series.groupby('id')['stand_amount'].apply(np.array) # returns a series\n",
    "series_list = [np.array(x) for x in series_list] # cast series into a list of np.arrays\n",
    "print(series_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then calculate the matrix\n",
    "distance_matrix = dtaidistance.dtw.distance_matrix_fast(series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, here is my code to do the pairwise calculations without a library\n",
    "# this is probably slower than dtaidistance.dtw.distance_matrix_fast \n",
    "# create an empty square dataframe to populate with distances\n",
    "# n_accounts = len(np.unique(standardized_series['id'])) \n",
    "# pairwise_dtw = pd.DataFrame(np.zeros([n_accounts, n_accounts])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the iterable\n",
    "# the_ids = standardized_series['id']\n",
    "\n",
    "# # loop through the iterable\n",
    "# for i, id1 in enumerate(the_ids):\n",
    "#     for j, id2 in enumerate(the_ids): # only populate top half \n",
    "#         # get the accounts' time series\n",
    "#         ts_one = standardized_series.loc[standardized_series['id'] == id1, ['timepoint', 'amount']]\n",
    "#         ts_two = standardized_series.loc[standardized_series['id'] == id2, ['timepoint', 'amount']]\n",
    "        \n",
    "#         # calculate Manhattan distance\n",
    "#         pairwise_dtw.loc[i,j] = fastdtw.fastdtw(ts_one, ts_two, dist=1)[0]\n",
    "#         print(i,id1, \"\\t\",j, id2)\n",
    "        \n",
    "#         # calculate Euclidean distance\n",
    "#         #pairwise_dtw.loc[i,j] = dtaidistance.dtw.distance(ts_one['amount'], ts_two['amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the simulated series in the wide format for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fix this if necessary, use the series_list above as a starting point\n",
    "## clustering wants the data in the wide format\n",
    "# n_rows = len([element for sim_list in all_series for element in sim_list])\n",
    "\n",
    "# all_series_wide = pd.DataFrame(index = range(0, n_rows), columns = ['id', 'timepoint', 'amount', 'process'])\n",
    "\n",
    "# for index in range(0, len(all_series)):\n",
    "#     all_series_wide.at[index, 'id'] = index\n",
    "#     all_series_wide.at[index, 'timepoint'] = list(range(0, len(all_series[index])))\n",
    "#     all_series_wide.at[index, 'amount'] = list(all_series[index])\n",
    "#     all_series_wide.at[index, 'process'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_series_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to warped paths, I could align in time and then calculate Euclidean distance timepoint by timepoint. This would be more appropriate if the series have seasonality - perhaps. It's also possible that if the series do have seasonality, they are offset relative to each other, for example in the case where winter-related products are relevant during different parts of the year e.g. Nov vs Jan. \n",
    "\n",
    "### Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two options: \n",
    "\n",
    "1. Use a supervised method by training a 1-NN classifier that uses DTW distances, 5-fold cross validation (based on sample size of 100) and simulated data so that the observations (time series) have labels (the process from which they were simulated eg. ARMA(1,1)). Hold out some of the simulated data to use as the test set, this represents the unlabeled \"real\" data. \n",
    "    \n",
    "    1-nn and dtw is difficult to beat for time series classification, and for a good summary of approaches taken towards time series classification including neural networks (predictive modular NNs, and a combination of HMM and ANNs) (page 83) [Mitsa et al 2010 Temporal Data Mining chapter on Temporal Classification](https://books.google.com/books?id=4P_7ydvW7cAC&pg=PA67&source=gbs_toc_r&cad=3#v=onepage&q&f=false)\n",
    "\n",
    "    A time series can be represented by a feature vector made up of the Fourier transform coefficients of the time series, and its mean, skewness, and kurtosis. This feature vector can be used as input to a clustering algorithm. \n",
    "\n",
    "    Also see [this paper](https://arxiv.org/pdf/1406.4757.pdf)\n",
    "\n",
    "2. Alternatively, use hierarchical clustering to identify groups of similar time series in an unsupervised way. The hope would be for time series generated by the same process, e.g. an ARMA(1,1) to be more similar to each other than to those generated by other processes, e.g. a MA(2). \n",
    "\n",
    "### With dtaidistance clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtaidistance import clustering\n",
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change inf to zeros\n",
    "distance_matrix[distance_matrix == inf] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  9.63114093,  7.38253416, ...,  7.83056531,\n",
       "        10.88216549,  7.75035625],\n",
       "       [ 0.        ,  0.        , 12.29826623, ..., 13.67602771,\n",
       "         7.86183125,  7.73503168],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  3.34815371,\n",
       "        12.85093749, 10.23867148],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        13.06055822, 10.92811713],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  8.26437202],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 1400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expected size is square, equal to the number of unique ids\n",
    "distance_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # computes the distance matrix first \n",
    "# model = dtaidistance.clustering.Hierarchical(dtaidistance.dtw.distance_matrix_fast, {})\n",
    "# model2 = dtaidistance.clustering.HierarchicalTree(model)\n",
    "# cluster_idx = model2.fit(distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>amount</th>\n",
       "      <th>stand_amount</th>\n",
       "      <th>process</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0ar1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.650442</td>\n",
       "      <td>-0.967444</td>\n",
       "      <td>ar1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0ar1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228639</td>\n",
       "      <td>-0.602090</td>\n",
       "      <td>ar1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0ar1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.293547</td>\n",
       "      <td>-0.159504</td>\n",
       "      <td>ar1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ar1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.557773</td>\n",
       "      <td>-0.928930</td>\n",
       "      <td>ar1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ar1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.404690</td>\n",
       "      <td>-0.528921</td>\n",
       "      <td>ar1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  timepoint    amount  stand_amount process\n",
       "0  0ar1          0 -0.650442     -0.967444     ar1\n",
       "1  0ar1          1  0.228639     -0.602090     ar1\n",
       "2  0ar1          2  1.293547     -0.159504     ar1\n",
       "3  0ar1          3 -0.557773     -0.928930     ar1\n",
       "4  0ar1          4  0.404690     -0.528921     ar1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146022, 5)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'n_clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b775beaf832a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstandardized_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_clusters' is not defined"
     ]
    }
   ],
   "source": [
    "print(standardized_series.shape)\n",
    "print(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_series['process'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster on the distance matrix\n",
    "n_clusters = len(np.unique(list(standardized_series['process']))) # use the real number of clusters \n",
    "hclust = AgglomerativeClustering(n_clusters = n_clusters, affinity='precomputed', linkage = 'average')\n",
    "clusters = hclust.fit(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a frequency table of the labels \n",
    "# np.bincount(clusters.labels_)\n",
    "np.unique(clusters.labels_, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the clusters\n",
    "# do they separate the processes? \n",
    "\n",
    "# merge in the ids for the learned labels\n",
    "membership = pd.DataFrame({\"id\": standardized_series['id'].unique(), \"hier_cluster\": clusters.labels_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "membership.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with the actual process labels to evaluate\n",
    "standardized_series = standardized_series.merge(membership, left_on=\"id\", right_on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the length of the series, keep just one record per time series\n",
    "# to compare real label with learned label\n",
    "per_id = standardized_series[standardized_series['timepoint'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_id = per_id.drop(['timepoint', 'amount', 'stand_amount'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many distinct ids have each (actual process & learned process) pair\n",
    "results = per_id.groupby(['process', 'hier_cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column id now shows the count of time series that were in class \"process\" (true label) \n",
    "# and clustered into \"hier_cluster\" (learned label)\n",
    "results = results.rename(columns = {'id': 'count'})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape so that all processes have all clusters represented in the dataframe, even if the counts are zero\n",
    "results = results.pivot(index='process', columns='hier_cluster', values='count')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.fillna(0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of this table: \n",
    "10 time series were (generated as ar1 processes) AND (clustered into cluster 0). \n",
    "etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each actual process, plot the distribution across the learned processes\n",
    "def plot_cluster(process_name, results_object):\n",
    "    plot1 = plt.bar(results_object.columns, results_object.loc[process_name])\n",
    "    plt.title(process_name + '\\n(total count = 100)')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Count')\n",
    "    plt.ylim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, process in enumerate(results.index):\n",
    "    plt.figure(index)\n",
    "    plot_cluster(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "* AR(1) processes are consistenly grouped into clusters 5 and 6\n",
    "* AR(2) processes, whether or not there are MA significant terms or differencing required (see ARIMA(2,1,1), ARIMA(2,2,2), ARMA(2,1) and ARMA(2,2)), are primarily in group 7\n",
    "* ARIMA processes are the cleanest grouping into group 10, however, ARIMA(0,1,1), ARIMA(1,1,0), ARIMA(1,1,1), and ARIMA(1,1,2) etc are indistinguishable from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try other linkage functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the above developed code in a function \n",
    "def cluster_and_compare(standardized_series, hclust_obj, col_name):\n",
    "    # evaluate the clusters\n",
    "    # do they separate the processes? \n",
    "\n",
    "    # cluster on the distance matrix\n",
    "    clusters = hclust_obj.fit(distance_matrix)\n",
    "\n",
    "    # merge in the ids for the learned labels\n",
    "    membership = pd.DataFrame({\"id\": standardized_series['id'].unique(), col_name: clusters.labels_})\n",
    "\n",
    "    # merge with the actual process labels to evaluate\n",
    "    merged = standardized_series.merge(membership, left_on=\"id\", right_on=\"id\")\n",
    "\n",
    "    # drop the length of the series, keep just one record per time series\n",
    "    # to compare real label with learned label\n",
    "    per_id = merged[merged['timepoint'] == 0]\n",
    "    per_id = per_id.drop(['timepoint', 'amount', 'stand_amount'], axis = 1)\n",
    "    \n",
    "    # count how many distinct ids have each (actual process & learned process) pair\n",
    "    results = per_id.groupby(['process', col_name]).count()\n",
    "    results = pd.DataFrame(results).reset_index()\n",
    "    # the column id now shows the count of time series that were in class \"process\" (true label) \n",
    "    # and clustered into \"hier_cluster\" (learned label)\n",
    "    results = results.rename(columns = {'id': 'count'})\n",
    "    # reshape so that all processes have all clusters represented in the dataframe, even if the counts are zero\n",
    "    results = results.pivot(index='process', columns=col_name, values='count')\n",
    "    results = results.fillna(0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete linkage with no n_cluster parameter specification  \n",
    "complete_hclust = AgglomerativeClustering(affinity='precomputed', linkage = 'complete')\n",
    "\n",
    "complete_results = cluster_and_compare(standardized_series, complete_hclust, 'complete_linkage_cluster')\n",
    "\n",
    "complete_results.columns = ['1', '2']\n",
    "\n",
    "for index, process in enumerate(complete_results.index):\n",
    "    plt.figure(index)\n",
    "    plot_cluster(process, complete_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete linkage with n_clusters = 14 parameter specification \n",
    "n_clusters = len(np.unique(list(standardized_series['process']))) # use the real number of clusters \n",
    "complete_hclust_14 = AgglomerativeClustering(n_clusters, affinity='precomputed', linkage = 'complete')\n",
    "\n",
    "complete_14_results = cluster_and_compare(standardized_series, complete_hclust_14, 'complete_14_cluster')\n",
    "\n",
    "complete_14_results.columns = ['1', '2']\n",
    "\n",
    "for index, process in enumerate(complete_14_results.index):\n",
    "    plt.figure(index)\n",
    "    plot_cluster(process, complete_14_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative clustering quality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering plus dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with scipy\n",
    "# scipy_hclust = linkage(pairwise_dtw, 'average')\n",
    "# fig = plt.figure(figsize=(25, 10))\n",
    "# dendro = dendrogram(hclust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy_hclust # what do these represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get cluster labels\n",
    "# results = dendrogram(scipy_hclust, \n",
    "#                     truncate_mode = 'lastp',  # show only the last p merged clusters\n",
    "#                     p = 4, # show only the last 4 merged clusters \n",
    "#                     no_plot = True) # returns the dictionary that contains the labels instead of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcluster(hclust, criterion='distance') # get cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to decompose the series and cluster only the trend components in an attempt to reduce the noise?\n",
    "# or go with the supervised classification approach \n",
    "# or ARMA processes are too similar to each other to tease apart the p and q parameters based on the geometry of the series - add ARCH/GARCH processes to the mix to see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use custom distance metric\n",
    "# dtw_metric = DistanceMetric.get_metric('pyfunc', func = fastdtw(dist = 1))\n",
    "# # 1-NN\n",
    "# knn = NearestNeighbors(n_neighbors = 1, radius = 0.5)#, metric = dtw_metric)\n",
    "# knn.fit(sim_series)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
